---
title: "Making an AI youtube shorts generator."
layout: "@layouts/blogLayout.astro"
date: "25/08/2024"
description: "An automated youtube channel completely handled by AI."
tags: ["AI", "Youtube"]
---
import Divider from "@components/Divider.astro"
import Paragraph from "@components/Paragraph.astro"
import { Image } from 'astro:assets';


import project from '@assets/blog/ai-shorts/project.png';
import thumbnail from '@assets/blog/ai-shorts/thumbnail.png';


<Paragraph title="Can Youtube Content be AI generated?" subtitle="This wasn't going to be easy.">
A few months ago, a friend messaged me about an idea he recently had: making a Youtube-Video-Shorts-Generator-3000 using AI, that would,
given a few information such as a video title and a body, generate a completely modern-style short video, and automatically upload it to youtube.
His goal was to start a fully automatic youtube channel, have it go viral, and eventually monetize it.

When he went up to me with this idea, I already knew he wasn't the first to think about this. In these last couple of years, with the rising of AI,
I've seen tons and tons of attempts at AI generated content, and had already thought of giving it a go myself. I decided to take this as an opportunity
to actually dip my hands into this type of projects, so we hopped on a call and started brainstorming an approach to the problem. In the end, we settled
for a generator that would fetch a specific pair of Post & Reply from the popular sub r/AskReddit, and generate a youtube short ready for upload.
</Paragraph>
<Paragraph title="How do you go from a Reddit Post to a Youtube Short?">
<Image src={project} alt="Our first logic map of the generator." class="shadow-md border-2 border-ebony-clay-950"/>
Converting from a text document to a well-edited & subtitled video is no easy task. Let's think about this problem, and break it up in single stepts that
a computer would need to do to accomplish this goal. First of all, we need to access reddit. Given a link to some content, using the Reddit API, we were able to
access a set of meta information about the post, such as the author, the title, the description, and even the replies. You can think about our "post" object
as something like the following:

```
test aaaaaaaaaaaaaaa aaaaaaa
```

After our post object was found and retrieved, we created a thumbnail for the post. This part was handled by my friend that worked on the project with me,
and basically created an image off of this POST element. Our goal was to display the thumbnail as the first part of the video, as if it was a screenshot
taken off reddit and edited in the short.

<Image src={thumbnail} alt="Our first logic map of the generator." class="shadow-md border-2 border-ebony-clay-950"/>

We felt that the number of upvotes & comments wasn't a really necessary detail, so we randomly generated those numbers.

</Paragraph>

<Paragraph title="How Did We Generate the Voice?">
As we moved forward, the next challenge was to generate a voiceover for our video that would resonate with viewers. We aimed to mimic that distinctive TikTok female voice, which had become synonymous with viral content. Unfortunately, after scouring various TTS (Text-to-Speech) services, we realized that none offered an affordable or even remotely accurate replication of that voice. Just when we were about to give up, I stumbled upon a website that used the exact voice we were looking for. This site exposed an API that allowed users to generate voice samples. While I'm not sure if the developer was aware of the API's potential, I decided to leverage it for our project.

There was a catch, though: the API had limitations on the length of the text it could process in one go. To work around this, I implemented a function that intelligently split the text into smaller chunks. Each chunk was carefully crafted to stay within the maximum allowed length, ensuring that the voice generation would not be cut off abruptly. These chunks were then processed individually, and the resulting audio files were stored in a specific folder structure: 

`./generated/<reddit_post_id>-<unix_date>/`

Within this folder, we stored both the thumbnail and the audio files. The audio files were named sequentially, like `sample1.wav`, `sample2.wav`, and so on, to keep everything organized and easily accessible. With this setup, we could efficiently manage and retrieve the assets needed for each video.

</Paragraph>

<Paragraph title="Synchronizing Subtitles with the Voice">
Once we had our audio files, the next step was to generate accurate subtitles. To achieve this, we merged all the audio samples into a single file, which allowed us to have one continuous voiceover. The key to creating engaging subtitles is precise timing, so we turned to OpenAI's Whisper model for this task. Whisper has a word-by-word mode that generates timing data for each spoken word, which was exactly what we needed.

We fed the combined audio file into Whisper, and it produced a detailed transcription along with timestamps for each word. This data was critical for the next phase of our project: creating animated subtitles. By having exact timings, we could ensure that each word appeared on screen at the perfect moment, enhancing the overall viewer experience and making the video more dynamic.

</Paragraph>